# -*- coding: utf-8 -*-
"""FinalProjectNewAutoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z5Z1bclyVbYVZXkSMCtjrRMFavlUOvuL
"""

!pip3 install torch 
!pip3 install torchvision
!pip3 install tqdm

from google.colab import drive
drive.mount('/content/gdrive')
import pandas as pd
import numpy as np
import gc
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from IPython import display
import matplotlib.pyplot as plt

policy1=pd.read_csv("/content/gdrive/My Drive/DeepLearning/PolicyDetails1.csv")
policy1=pd.read_csv("/content/gdrive/My Drive/DeepLearning/PolicyDetails1.csv")
claims1=pd.read_csv("/content/gdrive/My Drive/DeepLearning/ClaimAmounts1.csv")
claims2=pd.read_csv("/content/gdrive/My Drive/DeepLearning/ClaimAmounts2.csv")
policy1.head()

#features =pd.get_dummies(policy1.loc[:,['Power', 'Brand', 'Gas', 'Region', 'Density']])
#features.shape

# Save a module init file that contains a custom function that we'll use
# to verify that import works.


class PolicyDataset(Dataset):
    def __init__(self, filepath):
      #super.__init__(self)
      self.policy=pd.read_csv(filepath)
      self.policy['ClaimNb']=self.policy['ClaimNb'].apply(str)
      #print(type(self.policy.ClaimNb))

      self.policy_cat =pd.get_dummies(self.policy.loc[:,['ClaimNb', 'Power', 'Brand', 'Gas', 'Region']])
      self.policy_not_cat = self.policy.loc[:, ['Exposure', 'CarAge', 'DriverAge', 'Density']]
      #print(self.policy_cat.head())

      #print(self.policy.head())
      #add error checking to make sure we got the file we want
    def __getitem__(self,index):
      #print(self.policy.head())
      return torch.from_numpy(self.policy_not_cat.iloc[index].values).float(), torch.from_numpy(self.policy_cat.iloc[index].values).float()
      #Write a function that returns a tensor of the data
  
 
    def __len__(self):
      return len(self.policy.index)
    
    def getDatasetMeans(self):
      return(torch.from_numpy(self.policy_not_cat.mean(axis=0).values).cuda()).float()
      
    def getDatasetSDs(self):
      return(torch.from_numpy(self.policy_not_cat.std(axis=0).values).cuda()).float()

autoencoder=torch.load('/content/gdrive/My Drive/DeepLearning/autoencoder2')
file= "/content/gdrive/My Drive/DeepLearning/PolicyDetails1.csv"
data=PolicyDataset(file)
a,b=data[103]
#a, b = a.cuda(), b.cuda()
print(a, b)
print(autoencoder(b))
encoder=autoencoder.getEncoder()
encoder(b)

dataLoader = DataLoader(data, batch_size=1000, drop_last=True)
for i in dataLoader:
  apple=i
  break
apple.size()

class AutoEncode(nn.Module):
  def __init__(self):
    super(AutoEncode, self).__init__()
    self.net_d = nn.Sequential(nn.Linear(36,50),
                              nn.LeakyReLU(),
                              #nn.Linear(50, 50),
                              #nn.ReLU(),
                              nn.Linear(50,40))
    
  def forward(self, policies_d):
    return self.net_d(policies_d)
  
  
class AutoDecode(nn.Module):
  def __init__(self):
    super(AutoDecode, self).__init__()
    self.d_net = nn.Sequential(nn.Linear(40, 38),
                               nn.ReLU(),
                               nn.Linear(38,36),
                               nn.Sigmoid())
    
 
  def forward(self, policy):
    return self.d_net(policy)
  

  
  
  
class AutoEncoder(nn.Module):
  def __init__(self):
    super(AutoEncoder, self).__init__()
    self.c_encoder= AutoEncode()
    self.c_decoder=AutoDecode()
    
  def forward(self, policy_d):
    #return(policy, policy_d)
    return(self.c_decoder(self.c_encoder(policy_d)))

  def getEncoder(self):
    return(self.c_encoder)
  
  def getDecoder(self):
    return(self.c_decoder)
  

    

    
    
    
class AutoEncoderLoss(nn.Module):
  def __init__(self, data):
    super(AutoEncoderLoss, self).__init__()
    #self.means = data.getDatasetMeans()
    #self.sds = data.getDatasetSDs()
    #self.loss=nn.MSELoss()

    #print(self.means)
    #print(self.sds)
    
  def forward(self, original_d, decoded_d):
    
    #scaled_orig = torch.div(torch.sub(original, self.means), self.sds)
    #scaled_decoded = torch.div(torch.sub(decoded, self.means), self.sds)
    #print(scaled_orig)
    #print(scaled_orig.size())
    #print(scaled_decoded.size())
   # print(decoded)
   # print(decoded_d)
    
    cat_loss= -torch.sum(original_d*torch.log(torch.add(decoded_d, .000000001)) + (1-original_d) *torch.log(torch.add(1-decoded_d, .0000001)))

    return(cat_loss)

"""To do: split dataset so it returns two tensors: one with the one hot binary data and another with the non-binary data. Have two separate autoencoders, one trained on binary and the other not. Set up the regular loss as MSE (after standardizing?), set up the binary loss as the average of x*log(xhat) + (1-x)*log(1-xhat)."""

##Instantiate and RUN
learning_rate =.001
epoch=15
batch_size= 200

file= "/content/gdrive/My Drive/DeepLearning/PolicyDetails1.csv"
data=PolicyDataset(file)
dataLoader = DataLoader(data, batch_size=1000, drop_last=True)


autoencoder=AutoEncoder()
objective = AutoEncoderLoss(data)


loop =tqdm(total=len(dataLoader)*epoch, position=0, leave=False)
#for i in autoencoder.parameters():
#  print(i)
optimizer =  optim.Adam(autoencoder.parameters(), lr = learning_rate, eps=.01)
losses=[]
i=0
onePass=False
for e in range(epoch):
  for batch, batch_d in dataLoader:
    #print(batch_b.size())
    i+=1
    #print(batch.size())
    loop.update()
    optimizer.zero_grad()
    x_hat= autoencoder(batch_d)
    loss = objective(batch_d, x_hat)
    loss.backward()
    losses.append(loss.item())
    #clip_grad_norm_(autoencoder.get_parms(), 50000)
    optimizer.step()
    loop.set_description('loss:{}'.format(loss.item()))
    if i ==300:
      i=0
      onePass=True
      display.display(plt.gcf())
      display.clear_output(wait=True)
      plt.plot(losses[200:], label="Train")
      plt.title("Loss over time")
      plt.ylabel("Loss")
      plt.xlabel("Time")
      plt.legend()
      plt.show()
    if onePass and loss.item()>50000:
      for d in autoencoder.parameters():
        print(d.size())
        print(d)
      print(batch)
      print(batch_d)
      print(x_hat)


loop.close()

losses=[]
i=0
for e in range(epoch):
  for batch, batch_d in dataLoader:
    #print(batch_b.size())
    i+=1
    #print(batch.size())
    loop.update()
    x_hat= autoencoder( batch_d)
    loss = objective(batch_d, x_hat)
    losses.append(loss.item())
    #clip_grad_norm_(autoencoder.get_parms(), 50000)
    #optimizer.step()
    loop.set_description('loss:{}'.format(loss.item()))
    if i ==100:
      i=0
      onePass=True
      display.display(plt.gcf())
      display.clear_output(wait=True)
      plt.plot(losses, label="Train")
      plt.title("Loss over time")
      plt.ylabel("Loss")
      plt.xlabel("Time")
      plt.legend()
      plt.show()

for i in autoencoder.parameters():
  print(i.size())
  print(i)

plt.plot(losses[1600:], label="Train")
plt.title("Loss over time")
plt.ylabel("Loss")
plt.xlabel("Time")
plt.legend()
plt.show()
#print(losses[4700:])
#print(losses[9000:10000])

torch.save(autoencoder, f="/content/gdrive/My Drive/DeepLearning/autoencoder2")

  # .. to load your previously training model:
#autoencoder=torch.load('/content/gdrive/My Drive/DeepLearning/autoencoder2')

a, c=next(iter(dataLoader))
#print(a)
b=autoencoder(c)
#print(b[1])



print(b)
print(c)
objective(c, b)

enc=autoencoder.getEncoder()
encoded=enc(a, c)
print(encoded[0].size())