# -*- coding: utf-8 -*-
"""FinalProject2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ecQSz66EN_svfsWpnL6iv9GMWab8bW0e
"""

!pip3 install torch 
!pip3 install torchvision
!pip3 install tqdm

from google.colab import drive
drive.mount('/content/gdrive')
import pandas as pd
import numpy as np
import gc
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from IPython import display
import matplotlib.pyplot as plt
import torch.autograd as autograd

class PolicyDataset(Dataset):
    def __init__(self, filepath):
      #super.__init__(self)
      self.policy=pd.read_csv(filepath)
      self.policy['ClaimNb']=self.policy['ClaimNb'].apply(str)
      self.policy_cat =pd.get_dummies(self.policy.loc[:,['ClaimNb', 'Power', 'Brand', 'Gas', 'Region']])
      self.policy_not_cat = self.policy.loc[:, ['Exposure', 'CarAge', 'DriverAge', 'Density']]
      #self.policy_not_cat.ClaimFreq = self.policy_not_cat.ClaimNb/self.policy_not_cat.Exposure
      self.dataMean=torch.from_numpy(self.policy_not_cat.mean(axis=0).values).float()
      self.dataSd=torch.from_numpy(self.policy_not_cat.std(axis=0).values).float()
      print(self.dataMean)
      print(self.dataSd)
      #print(self.policy.head())
      #add error checking to make sure we got the file we want
    def __getitem__(self,index):
      #print(torch.sub(torch.from_numpy(self.policy_not_cat.iloc[index].values).float(), self.dataMean))
      return torch.div(torch.sub(torch.from_numpy(self.policy_not_cat.iloc[index].values).float(), self.dataMean), self.dataSd), torch.from_numpy(self.policy_cat.iloc[index].values).float()
      #Write a function that returns a tensor of the data
  
 
    def __len__(self):
      return len(self.policy.index)
    
    def getDatasetMeans(self):
      return(self.dataMean)
      
    def getDatasetSDs(self):
      return(self.dataSd)
    
    def colnames(self):
      return self.policy_cat.columns.values
    
    
    
class AutoEncode(nn.Module):
  def __init__(self):
    super(AutoEncode, self).__init__()
    self.net_d = nn.Sequential(nn.Linear(36,50),
                              nn.LeakyReLU(),
                              #nn.Linear(50, 50),
                              #nn.ReLU(),
                              nn.Linear(50,40))
    
  def forward(self, policies_d):
    return self.net_d(policies_d)
  
  
class AutoDecode(nn.Module):
  def __init__(self):
    super(AutoDecode, self).__init__()
    self.d_net = nn.Sequential(nn.Linear(40, 38),
                               nn.ReLU(),
                               nn.Linear(38,36),
                               nn.Sigmoid())
 
  def forward(self, policy):
    return self.d_net(policy)
  

  
  
  
class AutoEncoder(nn.Module):
  def __init__(self):
    super(AutoEncoder, self).__init__()
    self.c_encoder= AutoEncode()
    self.c_decode=AutoDecode()
    
  def forward(self, policy_d):
    #return(policy, policy_d)
    return(self.c_decode(self.c_encoder(policy_d)))

  def getEncoder(self):
    return(self.c_encoder)
  
  def getDecoder(self):
    return(self.c_decoder)

autoencoder=torch.load('/content/gdrive/My Drive/DeepLearning/autoencoder2')

class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    self.net = nn.Sequential(nn.Linear(120, 108),
                            nn.ReLU(),
                            #nn.BatchNorm1d(108),
                            nn.Linear(108, 70),
                            nn.ReLU(),
                            #nn.BatchNorm1d(70),
                            nn.Linear(70, 40),
                            nn.ReLU(),
                            #nn.BatchNorm1d(40),
                            nn.Linear(40, 10),
                            nn.ReLU(),
                            #nn.BatchNorm1d(10),

                            nn.Linear(10,1))
    
  def forward(self, batch):
    #print("onward Christian soldier")
    return(self.net(batch))
  
    
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    self.net=nn.Sequential(nn.Linear(50,40),
                           nn.ReLU(),
                          # nn.BatchNorm1d(40),

                           nn.Linear(40, 20),
                           nn.ReLU(),
                           #nn.BatchNorm1d(20),
                           nn.Linear(20, 10),
                           nn.ReLU(),
                           #nn.BatchNorm1d(10),
                           nn.Linear(10,4)
                          )
    self.discNet=nn.Sequential(nn.Linear(50, 60),
                               nn.ReLU(),
                               #nn.BatchNorm1d(60),
                               nn.Linear(60, 45),
                               nn.ReLU(),
                               #nn.BatchNorm1d(45),
                               nn.Linear(45,45),
                               nn.ReLU(),
                               #nn.BatchNorm1d(45),
                               nn.Linear(45, 40))
    
    
  def forward(self, zvec):
    #print("Forward into battle")
    #print(zvec.size())
    zCont, zDisc = torch.split(zvec, 50, dim=1)
    a=self.net(zCont)
    b= self.discNet(zDisc)
    #print(a.size())
    return(a, b)
    
class DiscriminatorLoss(nn.Module):
  def __init__(self, lamb, discriminator):
    super(DiscriminatorLoss, self).__init__()
    self.lamb=lamb
    self.discriminator=discriminator
    
  def forward(self,xfake, xreal, xmix):
    xmix=autograd.Variable(xmix, requires_grad=True)
    dxmix = self.discriminator(xmix)
    #https://github.com/caogang/wgan-gp/blob/master/gan_language.py
    gradients = autograd.grad(outputs=dxmix, inputs=xmix,
                  grad_outputs=torch.ones(dxmix.size()),
                  create_graph=True, retain_graph=True, only_inputs=True)[0]
    #print(gradients.size())
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    fakeScore=self.discriminator(xfake)
    realScore=self.discriminator(xreal)
    return(torch.mean(fakeScore-realScore+self.lamb*gradient_penalty), torch.mean(fakeScore), torch.mean(realScore))
    
class GeneratorLoss(nn.Module):
  def __init__(self, generator, discriminator):
    super(GeneratorLoss, self).__init__()
    #self.generator= generator
    self. discriminator=discriminator
    
  def forward(self, fake_x):
    return(torch.mean(-self.discriminator(fake_x)))

"""The goal of this function will be to take a minibatch of tensors, calculate some summary statistics about the whole batch, and attach each statistic to each example so it can be used by the discriminator"""

def attach_statistics(minibatch):
  minibatch=torch.cat(minibatch, dim=1)
  batch_size=minibatch.shape[0]
  means = minibatch.mean(0)
  means=torch.cat([means.unsqueeze(0)]*batch_size)
  variances= minibatch.var(0)
  variances=torch.cat([variances.unsqueeze(0)]*batch_size)
  withStats=torch.cat((minibatch, means, variances), dim=1)
  return(withStats)

def generate_zs(n, size):
  return(torch.from_numpy(np.random.normal(size=n*size)).float().view(-1,size))

def generate_eps(n):
  return(torch.from_numpy(np.random.uniform(size=n)).float().view(n,1))

def unnormalize(batchC):
  means= data.getDatasetMeans()
  sds = data.getDatasetSDs()
  #print(means, sds, batchC)
  return(torch.add(torch.mul(batchC, sds), means))

##Write a function that takes a generated batch and explains what it contains in a human readable way.
def printSample(batch):
  print("\n")
  print(['Exposure', 'CarAge', 'DriverAge', 'Density'])
  np.set_printoptions(suppress=True)
  print(np.round(unnormalize(batch[0]).detach().numpy()[0], 3))
  ##split up the tensor, find the softmax of each grouping.
  discrete = torch.split(batch[1], [5,12,7,2,10], dim=1)
  names =data.colnames()
  print(names[discrete[0].argmax(dim=1).item()])
  print("Confidence:", discrete[0].max()/discrete[0].sum().item())
  print(names[discrete[1].argmax(dim=1).item()+12])
  print("Confidence:", discrete[1].max()/discrete[1].sum().item())

  print(names[discrete[2].argmax(dim=1).item()+19])
  print("Confidence:", discrete[2].max()/discrete[2].sum().item())

  print(names[discrete[3].argmax(dim=1).item()+21])
  print("Confidence:", discrete[3].max()/discrete[3].sum().item())
  
def printBatchStats(batch):
  data, means, variances = torch.split(torch.chunk(batch, 1000, 0)[0], [40, 40, 40], dim=1)
  print("Means:", means)
  print("Variances:", variances)

print(vars(autoencoder))

epochs = 20
batch_size = 200
beta1 = 0.5 # 0
beta2 = 0.9 # 0.9
lamb= 10
ncritic =  1
learning_rate = 0.0002 # 0.0001


file= "/content/gdrive/My Drive/DeepLearning/PolicyDetails1.csv"
data=PolicyDataset(file)
dataLoader = DataLoader(data, batch_size=batch_size, drop_last=True, shuffle=True)


#discriminator = Discriminator()#.cuda()
discriminator=torch.load('/content/gdrive/My Drive/DeepLearning/discriminator')

#generator=Generator()#.cuda()
generator=torch.load('/content/gdrive/My Drive/DeepLearning/generator')


disc_loss = DiscriminatorLoss(lamb, discriminator)
gen_loss = GeneratorLoss(generator, discriminator)

decoder = autoencoder.getDecoder().to("cpu")

#initialize separate optimizer for both gen and disc
#d_optim =  optim.Adam(discriminator.parameters(), lr = learning_rate, betas=(beta1, beta2))
d_optim=torch.load('/content/gdrive/My Drive/DeepLearning/trained_optim_disc')

#g_optim = optim.Adam(generator.parameters(), lr = learning_rate, betas=(beta1, beta2))
g_optim=torch.load('/content/gdrive/My Drive/DeepLearning/trained_optim_gen')

gen_losses=[]
disc_losses = []
print_every =50 
pc=0

i=0
for g in generator.parameters():
  g.requires_grad=False
  
loop = tqdm(total=epochs*len(data)/batch_size, position = 0, leave=False)

fs=torch.zeros(1)
for e in range(epochs): ##Line 1
   for batch in dataLoader: #line 3, a batch being size m
      d_optim.zero_grad()
      i+=1
      batch = attach_statistics(batch) #line 4
      zvec =generate_zs(batch_size, 100) #line 4
      eps = generate_eps(batch_size) #line 4
      cont, disc = generator(zvec)
      x_fake = attach_statistics((cont,decoder(disc))) #Line 5 with medgan
      
      x_mix = eps * batch + (1-eps)*x_fake
      #print(batch.size())
      #print(x_fake.size())
      
      d_loss, fakeScore, realScore=disc_loss(x_fake, batch, x_mix)
      disc_losses.append(d_loss.item())
      d_loss.backward()
      d_optim.step()
      
      if i ==ncritic: #Line 2
        for g in generator.parameters():
          g.requires_grad=True
        for d in discriminator.parameters():
          d.requires_grad=False
        
        g_optim.zero_grad()
        i = 0 
        pc+=1
        zvec = generate_zs(batch_size, 100) #line 11
        cont, disc = generator(zvec)
        fake_x =  attach_statistics((cont, decoder(disc))) #line 12
        loss=gen_loss(fake_x) #line 12
        #garbage, fs, rs = disc_loss(fake_x, batch, x_mix)
        gen_losses.append(loss.item())
        loss.backward() #line 12
        g_optim.step() #line 12
        
        if pc == print_every:
          pc=0
          display.display(plt.gcf())
          display.clear_output(wait=True)
          plt.plot(gen_losses)
          plt.show()
          plt.plot(disc_losses)
          plt.show()
          #zvec=generate_zs(1, 100)
          #cont, disc = generator(zvec)
          #a=(cont, decoder(disc))
          #printSample(a)
          #zvec = generate_zs(1000, 100)
          #fakeBatch= attach_statistics(generator(zvec))
          print("Generated stats:")
          printBatchStats(fake_x)
          print("Real batch stats")
          printBatchStats(batch)
          #torch.save(discriminator, f="/content/gdrive/My Drive/DeepLearning/discriminator")
          #torch.save(generator, f="/content/gdrive/My Drive/DeepLearning/generator")
          #torch.save(d_optim, f="/content/gdrive/My Drive/DeepLearning/trained_optim_disc")
          #torch.save(g_optim, f="/content/gdrive/My Drive/DeepLearning/trained_optim_gen")
        
        for g in generator.parameters():
          g.requires_grad=False
        for d in discriminator.parameters():
          d.requires_grad=True
          
      loop.set_description('Epoch: % 1.0f Real Score: %3.4f Fake Score: % 3.4f' % (e, realScore.item(), fakeScore.item()))
      loop.update(1)

def unnormalize(batchC):
  means= data.getDatasetMeans()
  sds = data.getDatasetSDs()
  #print(means, sds, batchC)
  return(torch.add(torch.mul(batchC, sds), means))

##Write a function that takes a generated batch and explains what it contains in a human readable way.
def printSample(batch):
  print("\n")
  print(['ClaimNb', 'Exposure', 'CarAge', 'DriverAge', 'Density'])
  np.set_printoptions(suppress=True)
  print(np.round(unnormalize(batch[0]).detach().numpy()[0], 3))
  ##split up the tensor, find the softmax of each grouping.
  discrete = torch.split(batch[1],[5,12,7,2,10], dim=1)
  names =data.colnames()
  print(names[discrete[0].argmax(dim=1).item()])
  print("Confidence:", discrete[0].max()/discrete[0].sum().item())
  print(names[discrete[1].argmax(dim=1).item()+12])
  print("Confidence:", discrete[1].max()/discrete[1].sum().item())

  print(names[discrete[2].argmax(dim=1).item()+19])
  print("Confidence:", discrete[2].max()/discrete[2].sum().item())

  print(names[discrete[3].argmax(dim=1).item()+21])
  print("Confidence:", discrete[3].max()/discrete[3].sum().item())
  
  
  #print(discrete)
  #print(names)
  #print(discrete[1])
  #print(discrete[1].argmax(dim=1).item())
  
#print(a[0].size())
#print(a[1].size())
printSample(a)

def printBatchStats(batch):
  data,  variances = torch.split(torch.chunk(batch, 1000, 0)[0], [40,40], dim=1)
  #print("Means:", means)
  #print("Variances:", variances)
  
printBatchStats(fake_x)

def makePandas(cont, disc):
  print(cont.size())
  print(disc.size())
  cont_unnorm= unnormalize(cont).numpy()
  print(cont_unnorm.shape)
  df=pd.DataFrame(data=cont_unnorm, columns=['Exposure', 'Car Age','Driver Age','Density'])
  discrete = torch.split(disc,[5,12,7,2,10], dim=1)
  for i in range(5):
    cat=discrete[i].argmax(dim=1).numpy()
    column= pd.DataFrame(data=data.colnames()[cat+[0, 5, 17, 24, 26, 36][i]], columns=[['Claims', 'Power','Brand','Gas','Region'][i]])
    df=df.join(column)
  df.to_csv('/content/gdrive/My Drive/DeepLearning/generated2.csv')
  print(df.head())

zvec = generate_zs(10000, 100)
cont, disc =generator(zvec)
disc_d=decoder(disc)
df=makePandas(cont, disc_d)
fakeBatch= attach_statistics((cont, disc_d)).detach()
real=next(iter(dataLoader))
real=attach_statistics(real)
#your_file = open('/content/gdrive/My Drive/DeepLearning/fakeData.txt', 'ab')
#np.savetxt(your_file, fakeBatch.detach().numpy())
#your_file.close()

real_discriminants = discriminator(real)
fake_discriminants = discriminator(fakeBatch)
fake_data = fake_discriminants.view(1, 10000).detach().numpy()[0]
idx = np.argwhere(fake_data>-.8)
#print(cont[idx].size())
#df=makePandas(cont[idx].squeeze(1), disc_d[idx].squeeze(1))
real_data = real_discriminants.view(1,200).detach().numpy()[0]
from matplotlib import pyplot
bins = np.linspace(-5, 5, 100)

pyplot.hist(fake_data, bins, alpha=0.5, label='FAKE')
pyplot.hist(real_data, bins, alpha=0.5, label='REAL')
pyplot.legend(loc='upper right')
pyplot.show()

def makePandas(cont, disc):
  print(cont.size())
  print(disc.size())
  cont_unnorm= unnormalize(cont).detach().numpy()
  df=pd.DataFrame(data=cont_unnorm, columns=['Exposure', 'Car Age','Driver Age','Density'])
  discrete = torch.split(disc,[5,12,7,2,10], dim=1)
  for i in range(5):
    cat=discrete[i].argmax(dim=1).numpy()
    column= pd.DataFrame(data=data.colnames()[cat+[0, 5, 17, 24, 26, 36][i]], columns=[['Claims', 'Power','Brand','Gas','Region'][i]])
    df=df.join(column)
    df.to_csv('/content/gdrive/My Drive/DeepLearning/interpolation.csv')
    print(df.head())




startZ=generate_zs(1,100)
print(startZ)
newZ=startZ.detach().numpy()[0]
allZ=startZ
print(allZ.size())
for i in range(1000):
  newZ+=.01
  #print(torch.Tensor(newZ).unsqueeze(0).size())
  allZ=torch.cat((allZ, torch.Tensor(newZ).unsqueeze(0)), dim=0)
  #print(torch.Tensor(newZ))

#for i in range(100):
#  newZ[99-i]+=.2
#  allZ=torch.cat((allZ, torch.Tensor(newZ).unsqueeze(0)), dim=0)

cont, disc =generator(allZ)
disc_d=decoder(disc)
makePandas(cont, disc_d)